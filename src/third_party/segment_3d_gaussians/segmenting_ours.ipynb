{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d6c78206",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea6fa672",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "import torch\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from argparse import ArgumentParser, Namespace\n",
    "\n",
    "from arguments import ModelParams, PipelineParams\n",
    "from scene import Scene, GaussianModel, FeatureGaussianModel\n",
    "from gaussian_renderer import render, render_contrastive_feature\n",
    "\n",
    "def get_combined_args(parser : ArgumentParser, model_path, target_cfg_file = None):\n",
    "    cmdlne_string = ['--model_path', model_path]\n",
    "    cfgfile_string = \"Namespace()\"\n",
    "    args_cmdline = parser.parse_args(cmdlne_string)\n",
    "    \n",
    "    if target_cfg_file is None:\n",
    "        if args_cmdline.target == 'seg':\n",
    "            target_cfg_file = \"seg_cfg_args\"\n",
    "        elif args_cmdline.target == 'scene' or args_cmdline.target == 'xyz':\n",
    "            target_cfg_file = \"cfg_args\"\n",
    "        elif args_cmdline.target == 'feature' or args_cmdline.target == 'coarse_seg_everything' or args_cmdline.target == 'contrastive_feature' :\n",
    "            target_cfg_file = \"feature_cfg_args\"\n",
    "\n",
    "    try:\n",
    "        cfgfilepath = os.path.join(model_path, target_cfg_file)\n",
    "        print(\"Looking for config file in\", cfgfilepath)\n",
    "        with open(cfgfilepath) as cfg_file:\n",
    "            print(\"Config file found: {}\".format(cfgfilepath))\n",
    "            cfgfile_string = cfg_file.read()\n",
    "    except TypeError:\n",
    "        print(\"Config file found: {}\".format(cfgfilepath))\n",
    "        pass\n",
    "    args_cfgfile = eval(cfgfile_string)\n",
    "\n",
    "    merged_dict = vars(args_cfgfile).copy()\n",
    "    for k,v in vars(args_cmdline).items():\n",
    "        if v != None:\n",
    "            merged_dict[k] = v\n",
    "\n",
    "    return Namespace(**merged_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4384f5ab",
   "metadata": {},
   "source": [
    "## Hyper-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8c4629e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "FEATURE_DIM = 32 # fixed\n",
    "\n",
    "# MODEL_PATH = './output/lerf-fruit_aisle/'\n",
    "MODEL_PATH = '.../results/splatting_models/room1_final' # 30000\n",
    "\n",
    "FEATURE_GAUSSIAN_ITERATION = 10000\n",
    "\n",
    "SCALE_GATE_PATH = os.path.join(MODEL_PATH, f'point_cloud/iteration_{str(FEATURE_GAUSSIAN_ITERATION)}/scale_gate.pt')\n",
    "\n",
    "FEATURE_PCD_PATH = os.path.join(MODEL_PATH, f'point_cloud/iteration_{str(FEATURE_GAUSSIAN_ITERATION)}/contrastive_feature_point_cloud.ply')\n",
    "SCENE_PCD_PATH = os.path.join(MODEL_PATH, f'point_cloud/iteration_{str(FEATURE_GAUSSIAN_ITERATION)}/scene_point_cloud.ply')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb5b4b63",
   "metadata": {},
   "source": [
    "## Data and Model Preparation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "316ebd27",
   "metadata": {},
   "outputs": [],
   "source": [
    "scale_gate = torch.nn.Sequential(\n",
    "    torch.nn.Linear(1, 32, bias=True),\n",
    "    torch.nn.Sigmoid()\n",
    ")\n",
    "\n",
    "scale_gate.load_state_dict(torch.load(SCALE_GATE_PATH))\n",
    "scale_gate = scale_gate.cuda()\n",
    "\n",
    "parser = ArgumentParser(description=\"Testing script parameters\")\n",
    "model = ModelParams(parser, sentinel=True)\n",
    "pipeline = PipelineParams(parser)\n",
    "parser.add_argument('--target', default='scene', type=str)\n",
    "\n",
    "args = get_combined_args(parser, MODEL_PATH)\n",
    "\n",
    "dataset = model.extract(args)\n",
    "\n",
    "# If use language-driven segmentation, load clip feature and original masks\n",
    "dataset.need_features = True\n",
    "\n",
    "# To obtain mask scales\n",
    "dataset.need_masks = True\n",
    "\n",
    "scene_gaussians = GaussianModel(dataset.sh_degree)\n",
    "\n",
    "feature_gaussians = FeatureGaussianModel(FEATURE_DIM)\n",
    "scene = Scene(dataset, scene_gaussians, feature_gaussians, load_iteration=-1, feature_load_iteration=FEATURE_GAUSSIAN_ITERATION, shuffle=False, mode='eval', target='contrastive_feature')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "200a47a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_scales = []\n",
    "for cam in scene.getTrainCameras():\n",
    "    all_scales.append(cam.mask_scales)\n",
    "all_scales = torch.cat(all_scales)\n",
    "\n",
    "upper_bound_scale = all_scales.max().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f07ca2d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "cameras = scene.getTrainCameras()\n",
    "print(\"There are\",len(cameras),\"views in the dataset.\")\n",
    "print(upper_bound_scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b9d07b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_img_camera_id = 0\n",
    "mask_img_camera_id = 0\n",
    "\n",
    "view = deepcopy(cameras[ref_img_camera_id])\n",
    "\n",
    "view.feature_height, view.feature_width = view.image_height, view.image_width\n",
    "img = view.original_image * 255\n",
    "img = img.permute([1,2,0]).detach().cpu().numpy().astype(np.uint8)\n",
    "\n",
    "bg_color = [0 for i in range(FEATURE_DIM)]\n",
    "background = torch.tensor(bg_color, dtype=torch.float32, device=\"cuda\")\n",
    "rendered_feature = render_contrastive_feature(view, feature_gaussians, pipeline.extract(args), background, norm_point_features=True, smooth_type = None)['render']\n",
    "feature_h, feature_w = rendered_feature.shape[-2:]\n",
    "\n",
    "\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45694e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    scale = torch.tensor([1.]).cuda()\n",
    "    gates = scale_gate(scale)\n",
    "    feature_with_scale = rendered_feature\n",
    "    feature_with_scale = feature_with_scale * gates.unsqueeze(-1).unsqueeze(-1)\n",
    "    scale_conditioned_feature = feature_with_scale.permute([1,2,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e55301a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_point_features(feature_gaussians):\n",
    "    point_features = feature_gaussians.get_point_features\n",
    "    scale_conditioned_point_features = torch.nn.functional.normalize(point_features, dim = -1, p = 2) * gates.unsqueeze(0)\n",
    "    normed_point_features = torch.nn.functional.normalize(scale_conditioned_point_features, dim = -1, p = 2)\n",
    "    sampled_point_features = scale_conditioned_point_features[torch.rand(scale_conditioned_point_features.shape[0]) > 0.98]\n",
    "    normed_sampled_point_features = sampled_point_features / torch.norm(sampled_point_features, dim = -1, keepdim = True)\n",
    "\n",
    "    return normed_point_features, normed_sampled_point_features\n",
    "\n",
    "normed_point_features, normed_sampled_point_features = preprocess_point_features(feature_gaussians)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a91f6c3c",
   "metadata": {},
   "source": [
    "-------------------------------------------\n",
    "\n",
    "## \"Our\" Code starts here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e5a8594",
   "metadata": {},
   "source": [
    "## Perform 3D Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eecaa2e",
   "metadata": {},
   "source": [
    "For each of the 200 views we have, get the image with resulting clusters from 3D clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd3e06ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import hdbscan\n",
    "\n",
    "# clustering\n",
    "clusterer = hdbscan.HDBSCAN(min_cluster_size=50, cluster_selection_epsilon=0.1)\n",
    "cluster_labels = clusterer.fit_predict(normed_sampled_point_features.detach().cpu().numpy())\n",
    "cluster_centers = torch.zeros(len(np.unique(cluster_labels)) - 1, normed_sampled_point_features.shape[-1])\n",
    "\n",
    "for i in range(1, len(np.unique(cluster_labels))):\n",
    "    cluster_centers[i - 1] = torch.nn.functional.normalize(normed_sampled_point_features[cluster_labels == i - 1].mean(dim=0), dim=-1)\n",
    "\n",
    "# segmenting with all labels\n",
    "seg_score = torch.einsum('nc,bc->bn', cluster_centers.cpu(), normed_point_features.cpu())\n",
    "\n",
    "#TODO: Cann we use the same color for the same cluster every time? OpenNerf has the SCANNET Colorlist, maybe this is a start? Lets get rid of the randomness\n",
    "np.random.seed(12)\n",
    "label_to_color = np.random.rand(100, 3)\n",
    "\n",
    "point_colors = label_to_color[seg_score.argmax(dim=-1).cpu().numpy()] #\n",
    "point_colors[seg_score.max(dim=-1)[0].detach().cpu().numpy() < 0.5] = (0, 0, 0)\n",
    "\n",
    "try:\n",
    "    scene_gaussians.roll_back()\n",
    "except:\n",
    "    pass\n",
    "\n",
    "print(len(np.unique(cluster_labels)))\n",
    "print(np.unique(cluster_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec8db282",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------------------------------------\n",
    "\n",
    "### Extract single cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eb90de0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_cluster(cluster_label, point_colors, seg_score):\n",
    "    # Create a copy of point_colors\n",
    "    cluster_colors = np.copy(point_colors)\n",
    "    \n",
    "    # Identify points belonging to the specified cluster\n",
    "    cluster_indices = seg_score.argmax(dim=-1).cpu().numpy() == cluster_label\n",
    "    \n",
    "    # Replace points not in the specified cluster with black\n",
    "    cluster_colors[~cluster_indices] = (0, 0, 0)\n",
    "    \n",
    "    return cluster_colors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7586c846",
   "metadata": {},
   "source": [
    "Visualize a single cluster in this image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03fa4622",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for i in range(len(np.unique(cluster_labels))):    \n",
    "    new_points = extract_cluster(i, point_colors, seg_score)\n",
    "    #print(new_points.shape)\n",
    "\n",
    "    #TODO: What exactly is this background color?\n",
    "    bg_color = [0 for i in range(FEATURE_DIM)]\n",
    "    background = torch.tensor(bg_color, dtype=torch.float32, device=\"cuda\")\n",
    "\n",
    "    rendered_seg_map = render(cameras[0], scene_gaussians, pipeline.extract(args), background, override_color=torch.from_numpy(new_points).cuda().float())['render']\n",
    "    #print(rendered_seg_map.shape)\n",
    "    img_seg = rendered_seg_map.permute([1,2,0]).detach().cpu().numpy()\n",
    "    #print(img_seg.shape)\n",
    "\n",
    "    #TODO: Check if the criterion that the img_seg > 0.3 is a valid measure to see if a cluster is in this picture\n",
    "    mask = img_seg > 0.1\n",
    "    if mask.ndim > 2:\n",
    "        mask = mask.any(axis=-1)\n",
    "    segmented_image = np.zeros_like(img)\n",
    "    segmented_image[mask] = img[mask]\n",
    "\n",
    "    images_sum = img_seg.sum(axis=2)\n",
    "    #TODO: This 1 is a magic number atm (we check if the sum over all three channels is over 1), we should find a better way to determine the threshold\n",
    "    masks = torch.where(torch.tensor(images_sum > 0.5), 1.0, 0.)\n",
    "\n",
    "    # Plot images\n",
    "    plt.subplot(1,3,1)\n",
    "    plt.imshow(mask)\n",
    "\n",
    "    plt.subplot(1,3,2)\n",
    "    plt.imshow(masks)\n",
    "\n",
    "    plt.subplot(1,3,3)\n",
    "    plt.imshow(segmented_image)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "411f5f1b",
   "metadata": {},
   "source": [
    "## Move individual masks for every cluster into one array  \n",
    "Desired Size: [37, 679, 1199]  \n",
    "This can then be matched to the output of OV-SEG, because for the reduced replica classes this output is [51, 679, 1199]\n",
    "\n",
    "Everything over 0.5 is left in the mask, everything else is thrown out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99e6e01c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_ids = np.unique(cluster_labels)\n",
    "cluster_masks = torch.empty([len(cluster_ids), img.shape[0], img.shape[1]])\n",
    "\n",
    "for id in cluster_ids:\n",
    "    new_points = extract_cluster(id, point_colors, seg_score)\n",
    "    bg_color = [0 for i in range(FEATURE_DIM)]\n",
    "    background = torch.tensor(bg_color, dtype=torch.float32, device=\"cuda\")\n",
    "    rendered_seg_map = render(cameras[0], scene_gaussians, pipeline.extract(args), background, override_color=torch.from_numpy(new_points).cuda().float())['render']\n",
    "    img_seg = rendered_seg_map.permute([1,2,0]).detach().cpu()\n",
    "    # mask = img_seg > 0.1\n",
    "    # if mask.ndim > 2:\n",
    "    #     mask = mask.any(axis=-1)\n",
    "    #TODO: This 1 is a magic number atm (we check if the sum over all three channels is over 1), we should find a better way to determine the threshold\n",
    "    images_sum = img_seg.sum(axis=2)\n",
    "    mask = torch.where(images_sum > 0.5, 1.0, 0.)\n",
    "    cluster_masks[id] = mask\n",
    "\n",
    "plt.imshow(cluster_masks[20].cpu().numpy())\n",
    "cluster_masks.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34526d61",
   "metadata": {},
   "source": [
    "This is now the constructed tensor for a single image. It contains the masks for every found cluster. This is exported to pickle for further use in the OV-SEG Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c570eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('.../segment_3d_gaussians/segmentation_res/cluster_masks.pickle', 'wb') as handle:\n",
    "    pickle.dump(cluster_masks, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaa23edb",
   "metadata": {},
   "source": [
    "## Compare and match clusters to OVSEG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11e64fc1",
   "metadata": {},
   "source": [
    "Match the SAGA Clusters to the OVSEG Predictions\n",
    "=> See other notebook in ov-seg/test.ipynb\n",
    "\n",
    "\n",
    "Run all the code in the other notebook and copy the resulting cluster_labels (named different here because this variable already exists)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4417fb1c",
   "metadata": {},
   "source": [
    "These semantics are copied AFTER running the test.ipynb of OVSeg, and were pasted here to see if the matching makes sense, if this notebook is restarted it might not make sense anymore due to reshuffeling of the clusters (We have to understand the randomness in SAGA and ideally get it to be reproducible for our own iterations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f817cb7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_semantics = ['bin',\n",
    " 'bed',\n",
    " 'floor',\n",
    " 'chair',\n",
    " 'blinds',\n",
    " 'vase',\n",
    " 'sofa',\n",
    " 'table',\n",
    " 'picture',\n",
    " 'rug',\n",
    " 'window',\n",
    " 'lamp',\n",
    " 'wall-plug',\n",
    " 'tv-stand',\n",
    " 'pillow',\n",
    " 'bench',\n",
    " 'pot',\n",
    " 'tv-screen',\n",
    " 'cabinet',\n",
    " 'pillar',\n",
    " 'blanket',\n",
    " 'door',\n",
    " 'cushion',\n",
    " 'basket',\n",
    " 'indoor-plant',\n",
    " 'vent',\n",
    " 'shelf',\n",
    " 'stool',\n",
    " 'desk',\n",
    " 'comforter',\n",
    " 'nightstand',\n",
    " 'plant-stand',\n",
    " 'ceiling',\n",
    " 'plate',\n",
    " 'monitor',\n",
    " 'pipe',\n",
    " 'wall',\n",
    " 'panel']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9408906",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "with open('.../results/saga_masks/room1_new/frame_00001_pred.pkl', 'rb') as handle:\n",
    "    cluster_masks_unpadded = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "632eb5af",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(cluster_masks_unpadded.shape[0]):\n",
    "    print(cluster_semantics[i])\n",
    "    plt.imshow(cluster_masks_unpadded[i])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dbebcba",
   "metadata": {},
   "outputs": [],
   "source": [
    "for id in cluster_ids:\n",
    "    print(cluster_semantics[id])\n",
    "    new_points = extract_cluster(id, point_colors, seg_score)\n",
    "    bg_color = [0 for i in range(FEATURE_DIM)]\n",
    "\n",
    "    background = torch.tensor(bg_color, dtype=torch.float32, device=\"cuda\")\n",
    "    rendered_seg_map = render(cameras[0], scene_gaussians, pipeline.extract(args), background, override_color=torch.from_numpy(new_points).cuda().float())['render']\n",
    "    \n",
    "    img_seg = rendered_seg_map.permute([1,2,0]).detach().cpu().numpy()\n",
    "\n",
    "    #NOTE: Hyperparamter zum masken\n",
    "    mask = img_seg > 0.3\n",
    "    if mask.ndim > 2:\n",
    "        mask = mask.any(axis=-1)\n",
    "    segmented_image = np.zeros_like(img)\n",
    "    segmented_image[mask] = img[mask]\n",
    "\n",
    "    plt.imshow(segmented_image)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26c0ebe4",
   "metadata": {},
   "source": [
    "## 3: Compare to ground-truth labels\n",
    "\n",
    "This works only in the opennerf env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b4fd9f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import open3d as o3d\n",
    "import numpy as np\n",
    "import pickle\n",
    "import torch\n",
    "import os\n",
    "\n",
    "scene = \"room1\"\n",
    "\n",
    "mesh_path = f\".../opennerf/data/nerfstudio/replica_{scene}/{scene}_mesh.ply\"\n",
    "scene_point_cloud = o3d.io.read_point_cloud(mesh_path)\n",
    "points = np.array(scene_point_cloud.points)\n",
    "points.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40f45d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "saga_masks_path = f\".../results/saga_masks/{scene}_new/frame_00001_pred.pkl\"\n",
    "with open(saga_masks_path, 'rb') as handle:\n",
    "    cluster_masks = pickle.load(handle)\n",
    "merged_masks = torch.argmax(cluster_masks, dim=0)\n",
    "print(cluster_masks.shape)#, cluster_masks)\n",
    "print(merged_masks.shape, merged_masks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6feac46e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir = f\".../results/saga_masks/{scene}_new/\"\n",
    "path_list = sorted(os.listdir(dir))\n",
    "semantic_masks_saga = torch.empty([len(path_list), merged_masks.shape[0], merged_masks.shape[1]])\n",
    "for idx, path in enumerate(path_list):\n",
    "    with open(os.path.join(dir, path), 'rb') as handle:\n",
    "        cluster_masks = pickle.load(handle)\n",
    "    merged_masks = torch.argmax(cluster_masks, dim=0)\n",
    "    semantic_masks_saga[idx] = merged_masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dfebf31",
   "metadata": {},
   "outputs": [],
   "source": [
    "semantic_masks_saga[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d38ea39",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_semantics_dict = {\n",
    "    \"0\":'bin',\n",
    "    \"1\":'bed',\n",
    "    \"2\":'floor',\n",
    "    \"3\":'chair',\n",
    "    \"4\":'blinds',\n",
    "    \"5\":'vase',\n",
    "    \"6\":'sofa',\n",
    "    \"7\":'table',\n",
    "    \"8\":'picture',\n",
    "    \"9\":'rug',\n",
    "    \"10\":'window',\n",
    "    \"11\": 'lamp',\n",
    "    \"12\": 'wall-plug',\n",
    "    \"13\": 'tv-stand',\n",
    "    \"14\": 'pillow',\n",
    "    \"15\": 'bench',\n",
    "    \"16\": 'pot',\n",
    "    \"17\": 'tv-screen',\n",
    "    \"18\": 'cabinet',\n",
    "    \"19\": 'pillar',\n",
    "    \"20\": 'blanket',\n",
    "    \"21\": 'door',\n",
    "    \"22\": 'cushion',\n",
    "    \"23\": 'basket',\n",
    "    \"24\": 'indoor-plant',\n",
    "    \"25\": 'vent',\n",
    "    \"26\": 'shelf',\n",
    "    \"27\": 'stool',\n",
    "    \"28\": 'desk',\n",
    "    \"29\": 'comforter',\n",
    "    \"30\": 'nightstand',\n",
    "    \"31\": 'plant-stand',\n",
    "    \"32\": 'ceiling',\n",
    "    \"33\": 'plate',\n",
    "    \"34\": 'monitor',\n",
    "    \"35\": 'pipe',\n",
    "    \"36\": 'wall',\n",
    "    \"37\": 'panel'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5b3ce65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Load the JSON mapping\n",
    "# mapping_path = f'.../results/class_assignments/{scene}_openseg.json'\n",
    "# with open(mapping_path, 'r') as f:\n",
    "#     mapping = json.load(f)\n",
    "mapping = cluster_semantics_dict\n",
    "\n",
    "print(type(mapping))\n",
    "\n",
    "# Define the reduced labels\n",
    "class_names_reduced = [\n",
    "    'wall',\n",
    "    'ceiling',\n",
    "    'floor',\n",
    "    'chair',\n",
    "    'blinds',\n",
    "    'sofa',\n",
    "    'table',\n",
    "    'rug',\n",
    "    'window',\n",
    "    'lamp',\n",
    "    'door',\n",
    "    'pillow',\n",
    "    'bench',\n",
    "    'tv-screen',\n",
    "    'cabinet',\n",
    "    'pillar',\n",
    "    'blanket',\n",
    "    'tv-stand',\n",
    "    'cushion',\n",
    "    'bin',\n",
    "    'vent',\n",
    "    'bed',\n",
    "    'stool',\n",
    "    'picture',\n",
    "    'indoor-plant',\n",
    "    'desk',\n",
    "    'comforter',\n",
    "    'nightstand',\n",
    "    'shelf',\n",
    "    'vase',\n",
    "    'plant-stand',\n",
    "    'basket',\n",
    "    'plate',\n",
    "    'monitor',\n",
    "    'pipe',\n",
    "    'panel',\n",
    "    'desk-organizer',\n",
    "    'wall-plug',\n",
    "    'book',\n",
    "    'box',\n",
    "    'clock',\n",
    "    'sculpture',\n",
    "    'tissue-paper',\n",
    "    'camera',\n",
    "    'tablet',\n",
    "    'pot',\n",
    "    'bottle',\n",
    "    'candle',\n",
    "    'bowl',\n",
    "    'cloth',\n",
    "    'switch',\n",
    "    ]\n",
    "\n",
    "semantic_class_labels = torch.empty_like(semantic_masks_saga)\n",
    "\n",
    "# This loop is HORRIBLY slow, but it works, but we NEED to find a better way to do this\n",
    "\n",
    "# # Iterate over each cluster index in semantic_masks_saga\n",
    "# for k in [0]:#range(semantic_masks_saga.shape[0]):\n",
    "#     for i in range(semantic_masks_saga.shape[1]):\n",
    "#         for j in range(semantic_masks_saga.shape[2]):\n",
    "#             # Get the cluster index\n",
    "#             cluster_index = semantic_masks_saga[k, i, j]\n",
    "\n",
    "#             # Get the corresponding cluster label from the JSON mapping\n",
    "#             cluster_label = mapping[str(int(cluster_index))]\n",
    "\n",
    "#             # Convert the cluster label to the corresponding label index\n",
    "#             label_index = class_names_reduced.index(cluster_label)\n",
    "\n",
    "#             # Update the value in semantic_masks_saga with the label index\n",
    "#             semantic_class_labels[k, i, j] = label_index\n",
    "#     print(k)\n",
    "\n",
    "\n",
    "# This should fix the horribly slow loop\n",
    "# Create a lookup table for the labels to indices mapping\n",
    "label_to_index = {label: idx for idx, label in enumerate(class_names_reduced)}\n",
    "\n",
    "# Convert the mapping to use indices instead of labels\n",
    "mapping_indices = {int(key): label_to_index[label] for key, label in mapping.items()}\n",
    "\n",
    "# Convert semantic_masks_saga to a NumPy array if it's not already\n",
    "semantic_masks_saga_np = semantic_masks_saga.cpu().numpy() if isinstance(semantic_masks_saga, torch.Tensor) else semantic_masks_saga\n",
    "\n",
    "# Create an array for the mapped indices\n",
    "mapped_indices = np.vectorize(mapping_indices.get)(semantic_masks_saga_np)\n",
    "\n",
    "# Convert back to a torch.Tensor if needed\n",
    "semantic_class_labels = torch.tensor(mapped_indices, device=semantic_masks_saga.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2781b16f",
   "metadata": {},
   "outputs": [],
   "source": [
    "semantic_class_labels[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e13399f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'.../results/semantic_labels/semantic_class_labels_sm_{scene}_openseg.pickle', 'wb') as handle:\n",
    "    pickle.dump(semantic_class_labels, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54ab2a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "# Assuming semantic_masks_saga is your input tensor of shape [45, 679, 1199]\n",
    "input_tensor = semantic_class_labels\n",
    "\n",
    "# Define the padding\n",
    "# (padding_left, padding_right, padding_top, padding_bottom)\n",
    "# (0, 1) adds one column to the right, and (0, 1) adds one row to the bottom\n",
    "padding = (0, 1, 0, 1)\n",
    "\n",
    "# Apply padding\n",
    "padded_tensor = F.pad(input_tensor, padding, \"constant\", 0)\n",
    "\n",
    "# Verify the new shape\n",
    "padded_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fde34a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_tensor[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "611b1e97",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'.../results/semantic_labels/semantic_class_labels_padded_sm_{scene}_openseg.pickle', 'wb') as handle:\n",
    "    pickle.dump(padded_tensor, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d11ada92",
   "metadata": {},
   "outputs": [],
   "source": [
    "saga_semantics_path = f\".../results/semantic_labels/semantic_class_labels_padded_sm_{scene}_openseg.pickle\"\n",
    "with open(saga_semantics_path, 'rb') as handle:\n",
    "    saga_semantics = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7836205f",
   "metadata": {},
   "outputs": [],
   "source": [
    "saga_semantics_img1 = padded_tensor[0]\n",
    "saga_semantics_img1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b0988a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "SCANNET_COLOR_MAP_200 = {\n",
    "0: (196., 51., 182.),   # 0\n",
    "1: (174., 199., 232.),  # 1\n",
    "2: (188., 189., 34.),   # 2\n",
    "3: (152., 223., 138.),  # 3\n",
    "4: (255., 152., 150.),  # 4\n",
    "5: (214., 39., 40.),    # 5\n",
    "6: (91., 135., 229.),   # 6\n",
    "7: (31., 119., 180.),   # 7\n",
    "8: (229., 91., 104.),   # 8\n",
    "9: (247., 182., 210.),  # 9\n",
    "10: (91., 229., 110.),  # 10\n",
    "11: (255., 187., 120.), # 11\n",
    "12: (141., 91., 229.),  # 12\n",
    "13: (112., 128., 144.), # 13\n",
    "14: (196., 156., 148.), # 14\n",
    "15: (197., 176., 213.), # 15\n",
    "16: (44., 160., 44.),   # 16\n",
    "17: (148., 103., 189.), # 17\n",
    "18: (229., 91., 223.),  # 18\n",
    "19: (219., 219., 141.), # 19\n",
    "20: (192., 229., 91.),  # 20\n",
    "21: (88., 218., 137.),  # 21\n",
    "22: (58., 98., 137.),   # 22\n",
    "23: (177., 82., 239.),  # 23\n",
    "24: (255., 127., 14.),  # 24\n",
    "25: (237., 204., 37.),  # 25\n",
    "26: (41., 206., 32.),   # 26\n",
    "27: (62., 143., 148.),  # 27\n",
    "28: (34., 14., 130.),   # 28\n",
    "29: (143., 45., 115.),  # 29\n",
    "30: (137., 63., 14.),   # 30\n",
    "31: (23., 190., 207.),  # 31\n",
    "32: (16., 212., 139.),  # 32\n",
    "33: (90., 119., 201.),  # 33\n",
    "34: (125., 30., 141.),  # 34\n",
    "35: (150., 53., 56.),   # 35\n",
    "36: (186., 197., 62.),  # 36\n",
    "37: (227., 119., 194.), # 37\n",
    "38: (38., 100., 128.),  # 38\n",
    "39: (120., 31., 243.),  # 39\n",
    "40: (154., 59., 103.),  # 40\n",
    "41: (169., 137., 78.),  # 41\n",
    "42: (143., 245., 111.), # 42\n",
    "43: (37., 230., 205.),  # 43\n",
    "44: (14., 16., 155.),   # 44\n",
    "45: (208., 49., 84.),   # 45\n",
    "46: (237., 80., 38.),   # 46\n",
    "47: (138., 175., 62.),  # 47\n",
    "48: (158., 218., 229.), # 48\n",
    "49: (38., 96., 167.),   # 49\n",
    "50: (190., 77., 246.),  # 50\n",
    "51: (0., 0., 0.),}     # 51"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76039253",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "saga_semantics_img1_colors = np.array([list(SCANNET_COLOR_MAP_200.values())[i] for i in saga_semantics_img1.flatten()], dtype=np.uint8)\n",
    "saga_semantics_img1_colors = saga_semantics_img1_colors.reshape(saga_semantics_img1.shape + (3,))\n",
    "plt.imshow(saga_semantics_img1_colors)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "005c4750",
   "metadata": {},
   "outputs": [],
   "source": [
    "saga_mask_path_new = f\".../results/saga_masks/room1_new/frame_00001_pred.pkl\"\n",
    "saga_mask_path_old = f\".../results/saga_masks/room1_final/frame_00001_pred.pkl\"\n",
    "with open(saga_mask_path_new, 'rb') as handle:\n",
    "    saga_masks_new = pickle.load(handle)\n",
    "with open(saga_mask_path_old, 'rb') as handle:\n",
    "    saga_masks_old = pickle.load(handle)\n",
    "saga_masks_new_merged = torch.argmax(saga_masks_new, dim=0)\n",
    "plt.imshow(saga_masks_new_merged)\n",
    "plt.show()\n",
    "saga_masks_old_merged = torch.argmax(saga_masks_old, dim=0)\n",
    "plt.imshow(saga_masks_old_merged)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "038180ed",
   "metadata": {},
   "source": [
    "After the evaluation is done you can check your metrics here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eec2f62",
   "metadata": {},
   "outputs": [],
   "source": [
    "scenes = ['room1']#['office0', 'office1', 'office2', 'office3', 'office4', 'room0', 'room1', 'room2'] \n",
    "\n",
    "valid_class_ids = [0, 3, 7, 8, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 22, 23,\n",
    "                   26, 29, 31, 34, 35, 37, 40, 44, 47, 52, 54, 56, 59, 60, 61,\n",
    "                   62, 63, 64, 65, 70, 71, 76, 78, 79, 80, 82, 83, 87, 88, 91,\n",
    "                   92, 93, 95, 97, 98]\n",
    "\n",
    "class_popularity = [1, 47, 18, 22, 13,  5, 37, 40, 50, 49, 24, 21, 30,  2, 43, 11,\n",
    "                    29,  4, 44, 17,  3, 46,  1, 38, 28, 23, 19, 16, 26, 36, 45,\n",
    "                    32,  0, 33, 25, 31, 27, 20, 48,  6,  8, 14, 35, 42, 10, 41,\n",
    "                    34,  7, 12,  9, 15, 39]\n",
    "num_classes = len(valid_class_ids)  # 51\n",
    "\n",
    "map_to_reduced = {\n",
    "93:\t0,\n",
    "31:\t1,\n",
    "40:\t2,\n",
    "20:\t3,\n",
    "12:\t4,\n",
    "76:\t5,\n",
    "80:\t6,\n",
    "98:\t7,\n",
    "97:\t8,\n",
    "47:\t9,\n",
    "37:\t10,\n",
    "61:\t11,\n",
    "8:\t12,\n",
    "87:\t13,\n",
    "18:\t14,\n",
    "60:\t15,\n",
    "11:\t16,\n",
    "88:\t17,\n",
    "29:\t18,\n",
    "10:\t19,\n",
    "92:\t20,\n",
    "7:\t21,\n",
    "78:\t22,\n",
    "59:\t23,\n",
    "44:\t24,\n",
    "34:\t25,\n",
    "26:\t26,\n",
    "54:\t27,\n",
    "71:\t28,\n",
    "91:\t29,\n",
    "63:\t30,\n",
    "3:\t31,\n",
    "64:\t32,\n",
    "52:\t33,\n",
    "62:\t34,\n",
    "56:\t35,\n",
    "35:\t36,\n",
    "95:\t37,\n",
    "13:\t38,\n",
    "15:\t39,\n",
    "22:\t40,\n",
    "70:\t41,\n",
    "83:\t42,\n",
    "17:\t43,\n",
    "82:\t44,\n",
    "65:\t45,\n",
    "14:\t46,\n",
    "19:\t47,\n",
    "16:\t48,\n",
    "23:\t49,\n",
    "79:\t50,\n",
    "-1: 51,\n",
    "-2: 51,\n",
    "256:51,\n",
    "}\n",
    "\n",
    "class_names_reduced = [\n",
    "    'wall',\n",
    "    'ceiling',\n",
    "    'floor',\n",
    "    'chair',\n",
    "    'blinds',\n",
    "    'sofa',\n",
    "    'table',\n",
    "    'rug',\n",
    "    'window',\n",
    "    'lamp',\n",
    "    'door',\n",
    "    'pillow',\n",
    "    'bench',\n",
    "    'tv-screen',\n",
    "    'cabinet',\n",
    "    'pillar',\n",
    "    'blanket',\n",
    "    'tv-stand',\n",
    "    'cushion',\n",
    "    'bin',\n",
    "    'vent',\n",
    "    'bed',\n",
    "    'stool',\n",
    "    'picture',\n",
    "    'indoor-plant',\n",
    "    'desk',\n",
    "    'comforter',\n",
    "    'nightstand',\n",
    "    'shelf',\n",
    "    'vase',\n",
    "    'plant-stand',\n",
    "    'basket',\n",
    "    'plate',\n",
    "    'monitor',\n",
    "    'pipe',\n",
    "    'panel',\n",
    "    'desk-organizer',\n",
    "    'wall-plug',\n",
    "    'book',\n",
    "    'box',\n",
    "    'clock',\n",
    "    'sculpture',\n",
    "    'tissue-paper',\n",
    "    'camera',\n",
    "    'tablet',\n",
    "    'pot',\n",
    "    'bottle',\n",
    "    'candle',\n",
    "    'bowl',\n",
    "    'cloth',\n",
    "    'switch',\n",
    "    '0',\n",
    "    ]\n",
    "\n",
    "class_names = [\n",
    "    '0',                # 0\n",
    "    'backpack',         # 1\n",
    "    'base-cabinet',     # 2\n",
    "    'basket',           # 3\n",
    "    'bathtub',          # 4\n",
    "    'beam',\n",
    "    'beanbag',\n",
    "    'bed',\n",
    "    'bench',\n",
    "    'bike',\n",
    "    'bin',\n",
    "    'blanket',\n",
    "    'blinds',\n",
    "    'book',\n",
    "    'bottle',\n",
    "    'box',\n",
    "    'bowl',\n",
    "    'camera',\n",
    "    'cabinet',\n",
    "    'candle',\n",
    "    'chair',\n",
    "    'chopping-board',\n",
    "    'clock',\n",
    "    'cloth',\n",
    "    'clothing',\n",
    "    'coaster',\n",
    "    'comforter',\n",
    "    'computer-keyboard',\n",
    "    'cup',\n",
    "    'cushion',\n",
    "    'curtain',\n",
    "    'ceiling',\n",
    "    'cooktop',\n",
    "    'countertop',\n",
    "    'desk',\n",
    "    'desk-organizer',\n",
    "    'desktop-computer',\n",
    "    'door',\n",
    "    'exercise-ball',\n",
    "    'faucet',\n",
    "    'floor',\n",
    "    'handbag',\n",
    "    'hair-dryer',\n",
    "    'handrail',\n",
    "    'indoor-plant',\n",
    "    'knife-block',\n",
    "    'kitchen-utensil',\n",
    "    'lamp',\n",
    "    'laptop',\n",
    "    'major-appliance',\n",
    "    'mat',\n",
    "    'microwave',\n",
    "    'monitor',\n",
    "    'mouse',\n",
    "    'nightstand',\n",
    "    'pan',\n",
    "    'panel',\n",
    "    'paper-towel',\n",
    "    'phone',\n",
    "    'picture',\n",
    "    'pillar',\n",
    "    'pillow',\n",
    "    'pipe',\n",
    "    'plant-stand',\n",
    "    'plate',\n",
    "    'pot',\n",
    "    'rack',\n",
    "    'refrigerator',\n",
    "    'remote-control',\n",
    "    'scarf',\n",
    "    'sculpture',\n",
    "    'shelf',\n",
    "    'shoe',\n",
    "    'shower-stall',\n",
    "    'sink',\n",
    "    'small-appliance',\n",
    "    'sofa',\n",
    "    'stair',\n",
    "    'stool',\n",
    "    'switch',\n",
    "    'table',\n",
    "    'table-runner',\n",
    "    'tablet',\n",
    "    'tissue-paper',\n",
    "    'toilet',\n",
    "    'toothbrush',\n",
    "    'towel',\n",
    "    'tv-screen',\n",
    "    'tv-stand',\n",
    "    'umbrella',\n",
    "    'utensil-holder',\n",
    "    'vase',\n",
    "    'vent',\n",
    "    'wall',\n",
    "    'wall-cabinet',\n",
    "    'wall-plug',\n",
    "    'wardrobe',\n",
    "    'window',\n",
    "    'rug',\n",
    "    'logo',\n",
    "    'bag',\n",
    "    'set-of-clothing',\n",
    "]\n",
    "\n",
    "SCANNET_COLOR_MAP_200 = {\n",
    "0: (196., 51., 182.),   # 0\n",
    "1: (174., 199., 232.),  # 1\n",
    "2: (188., 189., 34.),   # 2\n",
    "3: (152., 223., 138.),  # 3\n",
    "4: (255., 152., 150.),  # 4\n",
    "5: (214., 39., 40.),    # 5\n",
    "6: (91., 135., 229.),   # 6\n",
    "7: (31., 119., 180.),   # 7\n",
    "8: (229., 91., 104.),   # 8\n",
    "9: (247., 182., 210.),  # 9\n",
    "10: (91., 229., 110.),  # 10\n",
    "11: (255., 187., 120.), # 11\n",
    "13: (141., 91., 229.),  # 12\n",
    "14: (112., 128., 144.), # 13\n",
    "15: (196., 156., 148.), # 14\n",
    "16: (197., 176., 213.), # 15\n",
    "17: (44., 160., 44.),   # 16\n",
    "18: (148., 103., 189.), # 17\n",
    "19: (229., 91., 223.),  # 18\n",
    "21: (219., 219., 141.), # 19\n",
    "22: (192., 229., 91.),  # 20\n",
    "23: (88., 218., 137.),  # 21\n",
    "24: (58., 98., 137.),   # 22\n",
    "26: (177., 82., 239.),  # 23\n",
    "27: (255., 127., 14.),  # 24\n",
    "28: (237., 204., 37.),  # 25\n",
    "29: (41., 206., 32.),   # 26\n",
    "31: (62., 143., 148.),  # 27\n",
    "32: (34., 14., 130.),   # 28\n",
    "33: (143., 45., 115.),  # 29\n",
    "34: (137., 63., 14.),   # 30\n",
    "35: (23., 190., 207.),  # 31\n",
    "36: (16., 212., 139.),  # 32\n",
    "38: (90., 119., 201.),  # 33\n",
    "39: (125., 30., 141.),  # 34\n",
    "40: (150., 53., 56.),   # 35\n",
    "41: (186., 197., 62.),  # 36\n",
    "42: (227., 119., 194.), # 37\n",
    "44: (38., 100., 128.),  # 38\n",
    "45: (120., 31., 243.),  # 39\n",
    "46: (154., 59., 103.),  # 40\n",
    "47: (169., 137., 78.),  # 41\n",
    "48: (143., 245., 111.), # 42\n",
    "49: (37., 230., 205.),  # 43\n",
    "50: (14., 16., 155.),   # 44\n",
    "51: (208., 49., 84.),   # 45\n",
    "52: (237., 80., 38.),   # 46\n",
    "54: (138., 175., 62.),  # 47\n",
    "55: (158., 218., 229.), # 48\n",
    "56: (38., 96., 167.),   # 49\n",
    "57: (190., 77., 246.),  # 50\n",
    "58: (0., 0., 0.),       # 51\n",
    "59: (208., 193., 72.),\n",
    "62: (55., 220., 57.),\n",
    "63: (10., 125., 140.),\n",
    "64: (76., 38., 202.),\n",
    "65: (191., 28., 135.),\n",
    "66: (211., 120., 42.),\n",
    "67: (118., 174., 76.),\n",
    "68: (17., 242., 171.),\n",
    "69: (20., 65., 247.),\n",
    "70: (208., 61., 222.),\n",
    "71: (162., 62., 60.),\n",
    "72: (210., 235., 62.),\n",
    "73: (45., 152., 72.),\n",
    "74: (35., 107., 149.),\n",
    "75: (160., 89., 237.),\n",
    "76: (227., 56., 125.),\n",
    "77: (169., 143., 81.),\n",
    "78: (42., 143., 20.),\n",
    "79: (25., 160., 151.),\n",
    "80: (82., 75., 227.),\n",
    "82: (253., 59., 222.),\n",
    "84: (240., 130., 89.),\n",
    "86: (123., 172., 47.),\n",
    "87: (71., 194., 133.),\n",
    "88: (24., 94., 205.),\n",
    "89: (134., 16., 179.),\n",
    "90: (159., 32., 52.),\n",
    "93: (213., 208., 88.),\n",
    "95: (64., 158., 70.),\n",
    "96: (18., 163., 194.),\n",
    "97: (65., 29., 153.),\n",
    "98: (177., 10., 109.),\n",
    "99: (152., 83., 7.),\n",
    "100: (83., 175., 30.),\n",
    "101: (18., 199., 153.),\n",
    "102: (61., 81., 208.),\n",
    "103: (213., 85., 216.),\n",
    "104: (170., 53., 42.),\n",
    "105: (161., 192., 38.),\n",
    "106: (23., 241., 91.),\n",
    "107: (12., 103., 170.),\n",
    "110: (151., 41., 245.),\n",
    "112: (133., 51., 80.),\n",
    "115: (184., 162., 91.),\n",
    "116: (50., 138., 38.),\n",
    "118: (31., 237., 236.),\n",
    "120: (39., 19., 208.),\n",
    "121: (223., 27., 180.),\n",
    "122: (254., 141., 85.),\n",
    "125: (97., 144., 39.),\n",
    "128: (106., 231., 176.),\n",
    "130: (12., 61., 162.),\n",
    "131: (124., 66., 140.),\n",
    "132: (137., 66., 73.),\n",
    "134: (250., 253., 26.),\n",
    "136: (55., 191., 73.),\n",
    "138: (60., 126., 146.),\n",
    "139: (153., 108., 234.),\n",
    "140: (184., 58., 125.),\n",
    "141: (135., 84., 14.),\n",
    "145: (139., 248., 91.),\n",
    "148: (53., 200., 172.),\n",
    "154: (63., 69., 134.),\n",
    "155: (190., 75., 186.),\n",
    "156: (127., 63., 52.),\n",
    "157: (141., 182., 25.),\n",
    "159: (56., 144., 89.),\n",
    "161: (64., 160., 250.),\n",
    "163: (182., 86., 245.),\n",
    "165: (139., 18., 53.),\n",
    "166: (134., 120., 54.),\n",
    "168: (49., 165., 42.),\n",
    "169: (51., 128., 133.),\n",
    "170: (44., 21., 163.),\n",
    "177: (232., 93., 193.),\n",
    "180: (176., 102., 54.),\n",
    "185: (116., 217., 17.),\n",
    "188: (54., 209., 150.),\n",
    "191: (60., 99., 204.),\n",
    "193: (129., 43., 144.),\n",
    "195: (252., 100., 106.),\n",
    "202: (187., 196., 73.),\n",
    "208: (13., 158., 40.),\n",
    "213: (52., 122., 152.),\n",
    "214: (128., 76., 202.),\n",
    "221: (187., 50., 115.),\n",
    "229: (180., 141., 71.),\n",
    "230: (77., 208., 35.),\n",
    "232: (72., 183., 168.),\n",
    "233: (97., 99., 203.),\n",
    "242: (172., 22., 158.),\n",
    "250: (155., 64., 40.),\n",
    "261: (118., 159., 30.),\n",
    "264: (69., 252., 148.),\n",
    "276: (45., 103., 173.),\n",
    "283: (111., 38., 149.),\n",
    "286: (184., 9., 49.),\n",
    "300: (188., 174., 67.),\n",
    "304: (53., 206., 53.),\n",
    "312: (97., 235., 252.),\n",
    "323: (66., 32., 182.),\n",
    "325: (236., 114., 195.),\n",
    "331: (241., 154., 83.),\n",
    "342: (133., 240., 52.),\n",
    "356: (16., 205., 144.),\n",
    "370: (75., 101., 198.),\n",
    "392: (237., 95., 251.),\n",
    "395: (191., 52., 49.),\n",
    "399: (227., 254., 54.),\n",
    "408: (49., 206., 87.),\n",
    "417: (48., 113., 150.),\n",
    "488: (125., 73., 182.),\n",
    "540: (229., 32., 114.),\n",
    "562: (158., 119., 28.),\n",
    "570: (60., 205., 27.),\n",
    "572: (18., 215., 201.),\n",
    "581: (79., 76., 153.),\n",
    "609: (134., 13., 116.),\n",
    "748: (192., 97., 63.),\n",
    "776: (108., 163., 18.),\n",
    "1156: (95., 220., 156.),\n",
    "1163: (98., 141., 208.),\n",
    "1164: (144., 19., 193.),\n",
    "1165: (166., 36., 57.),\n",
    "1166: (212., 202., 34.),\n",
    "1167: (23., 206., 34.),\n",
    "1168: (91., 211., 236.),\n",
    "1169: (79., 55., 137.),\n",
    "1170: (182., 19., 117.),\n",
    "1171: (134., 76., 14.),\n",
    "1172: (87., 185., 28.),\n",
    "1173: (82., 224., 187.),\n",
    "1174: (92., 110., 214.),\n",
    "1175: (168., 80., 171.),\n",
    "1176: (197., 63., 51.),\n",
    "1178: (175., 199., 77.),\n",
    "1179: (62., 180., 98.),\n",
    "1180: (8., 91., 150.),\n",
    "1181: (77., 15., 130.),\n",
    "1182: (154., 65., 96.),\n",
    "1183: (197., 152., 11.),\n",
    "1184: (59., 155., 45.),\n",
    "1185: (12., 147., 145.),\n",
    "1186: (54., 35., 219.),\n",
    "1187: (210., 73., 181.),\n",
    "1188: (221., 124., 77.),\n",
    "1189: (149., 214., 66.),\n",
    "1190: (72., 185., 134.),\n",
    "1191: (42., 94., 198.),\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a18363a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "PREFIX = \".../opennerf\"\n",
    "experiment_name = \"benchmark\"\n",
    "scene = \"room1\"\n",
    "\n",
    "def process_txt(filename):\n",
    "    with open(filename) as file:\n",
    "        lines = file.readlines()\n",
    "        lines = [line.rstrip() for line in lines]\n",
    "    return lines\n",
    "\n",
    "def eval_semantics():\n",
    "\n",
    "    pr_files = []  # predicted files\n",
    "    gt_files = []  # ground truth files\n",
    "    # for scene in scenes:\n",
    "    #     pr_files.append(f'{PREFIX}/outputs/replica_{scene}/opennerf/{experiment_name}/semantics_{scene}.txt')\n",
    "    #     gt_files.append(f'{PREFIX}/datasets/replica_gt_semantics/semantic_labels_{scene}.txt')\n",
    "    pr_files.append(f'{PREFIX}/outputs/replica_{scene}/opennerf/{experiment_name}/semantics_{scene}.txt')\n",
    "    gt_files.append(f'{PREFIX}/datasets/replica_gt_semantics/semantic_labels_{scene}.txt')\n",
    "\n",
    "    confusion = np.zeros([num_classes, num_classes], dtype=np.ulonglong)\n",
    "\n",
    "    print('evaluating', len(pr_files), 'scans...')\n",
    "    for i in range(len(pr_files)):\n",
    "        evaluate_scan(pr_files[i], gt_files[i], confusion)\n",
    "        sys.stdout.write(\"\\rscans processed: {}\".format(i+1))\n",
    "        sys.stdout.flush()\n",
    "\n",
    "    class_ious = {}\n",
    "    for i in range(num_classes):\n",
    "        label_name = class_names_reduced[i]\n",
    "        label_id = i\n",
    "        class_ious[label_name] = get_iou(label_id, confusion)\n",
    "\n",
    "    print('classes \\t IoU \\t Acc')\n",
    "    print('----------------------------')\n",
    "    for i in range(num_classes):\n",
    "        label_name = class_names_reduced[i]\n",
    "        print('{0:<14s}: {1:>5.2%}   {2:>6.2%}'.format(label_name, class_ious[label_name][0], class_ious[label_name][1]))\n",
    "\n",
    "    iou_values = np.array([i[0] for i in class_ious.values()])\n",
    "    acc_values = np.array([i[1] for i in class_ious.values()])\n",
    "    print()\n",
    "    print(f'mIoU: \\t {np.mean(iou_values):.2%}')\n",
    "    print(f'mAcc: \\t {np.mean(acc_values):.2%}')\n",
    "    print()\n",
    "    for i, split in enumerate(['head', 'comm', 'tail']):\n",
    "        print(f'{split}: \\t {np.mean(iou_values[17 * i:17 * (i + 1)]):.2%}')\n",
    "        print(f'{split}: \\t {np.mean(acc_values[17 * i:17 * (i + 1)]):.2%}')\n",
    "        print('---')\n",
    "\n",
    "\n",
    "def evaluate_scan(pr_file, gt_file, confusion):\n",
    "\n",
    "    pr_ids = np.array(process_txt(pr_file), dtype=np.int64)\n",
    "    gt_file_contents = np.array(process_txt(gt_file)).astype(np.int64)\n",
    "    gt_ids = np.vectorize(map_to_reduced.get)(gt_file_contents)\n",
    "\n",
    "    # sanity checks\n",
    "    if not pr_ids.shape == gt_ids.shape:\n",
    "        print(f'number of predicted values does not match number of vertices: {pr_file}')\n",
    "    for (gt_val, pr_val) in zip(gt_ids, pr_ids):\n",
    "        if gt_val == num_classes:\n",
    "            continue\n",
    "        confusion[gt_val][pr_val] += 1\n",
    "\n",
    "\n",
    "def get_iou(label_id, confusion):\n",
    "    tp = np.longlong(confusion[label_id, label_id])\n",
    "    fn = np.longlong(confusion[label_id, :].sum()) - tp\n",
    "    fp = np.longlong(confusion[:, label_id].sum()) - tp\n",
    "    denom = float(tp + fp + fn)\n",
    "    if denom == 0:\n",
    "        return (0, 0) #float('nan')\n",
    "    iou = tp / denom\n",
    "\n",
    "    if tp==0 and fn==0:\n",
    "        return (iou, 0)\n",
    "    acc = tp / float(tp + fn)\n",
    "    \n",
    "    return (iou, acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9b29cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_semantics()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
