{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea6fa672",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "import torch\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from PIL import Image\n",
    "from argparse import ArgumentParser, Namespace\n",
    "import cv2\n",
    "# import cuml\n",
    "import hdbscan\n",
    "\n",
    "from arguments import ModelParams, PipelineParams\n",
    "from scene import Scene, GaussianModel, FeatureGaussianModel\n",
    "from gaussian_renderer import render, render_contrastive_feature\n",
    "\n",
    "def get_combined_args(parser : ArgumentParser, model_path, target_cfg_file = None):\n",
    "    cmdlne_string = ['--model_path', model_path]\n",
    "    cfgfile_string = \"Namespace()\"\n",
    "    args_cmdline = parser.parse_args(cmdlne_string)\n",
    "    \n",
    "    if target_cfg_file is None:\n",
    "        if args_cmdline.target == 'seg':\n",
    "            target_cfg_file = \"seg_cfg_args\"\n",
    "        elif args_cmdline.target == 'scene' or args_cmdline.target == 'xyz':\n",
    "            target_cfg_file = \"cfg_args\"\n",
    "        elif args_cmdline.target == 'feature' or args_cmdline.target == 'coarse_seg_everything' or args_cmdline.target == 'contrastive_feature' :\n",
    "            target_cfg_file = \"feature_cfg_args\"\n",
    "\n",
    "    try:\n",
    "        cfgfilepath = os.path.join(model_path, target_cfg_file)\n",
    "        print(\"Looking for config file in\", cfgfilepath)\n",
    "        with open(cfgfilepath) as cfg_file:\n",
    "            print(\"Config file found: {}\".format(cfgfilepath))\n",
    "            cfgfile_string = cfg_file.read()\n",
    "    except TypeError:\n",
    "        print(\"Config file found: {}\".format(cfgfilepath))\n",
    "        pass\n",
    "    args_cfgfile = eval(cfgfile_string)\n",
    "\n",
    "    merged_dict = vars(args_cfgfile).copy()\n",
    "    for k,v in vars(args_cmdline).items():\n",
    "        if v != None:\n",
    "            merged_dict[k] = v\n",
    "\n",
    "    return Namespace(**merged_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4384f5ab",
   "metadata": {},
   "source": [
    "# Hyper-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8c4629e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "FEATURE_DIM = 32 # fixed\n",
    "\n",
    "# MODEL_PATH = './output/lerf-fruit_aisle/'\n",
    "MODEL_PATH = '.../results/splatting_models/scene0000_00' # 30000\n",
    "\n",
    "FEATURE_GAUSSIAN_ITERATION = 10000\n",
    "\n",
    "SCALE_GATE_PATH = os.path.join(MODEL_PATH, f'point_cloud/iteration_{str(FEATURE_GAUSSIAN_ITERATION)}/scale_gate.pt')\n",
    "\n",
    "FEATURE_PCD_PATH = os.path.join(MODEL_PATH, f'point_cloud/iteration_{str(FEATURE_GAUSSIAN_ITERATION)}/contrastive_feature_point_cloud.ply')\n",
    "SCENE_PCD_PATH = os.path.join(MODEL_PATH, f'point_cloud/iteration_{str(FEATURE_GAUSSIAN_ITERATION)}/scene_point_cloud.ply')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb5b4b63",
   "metadata": {},
   "source": [
    "# Data and Model Preparation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "316ebd27",
   "metadata": {},
   "outputs": [],
   "source": [
    "scale_gate = torch.nn.Sequential(\n",
    "    torch.nn.Linear(1, 32, bias=True),\n",
    "    torch.nn.Sigmoid()\n",
    ")\n",
    "\n",
    "scale_gate.load_state_dict(torch.load(SCALE_GATE_PATH))\n",
    "scale_gate = scale_gate.cuda()\n",
    "\n",
    "parser = ArgumentParser(description=\"Testing script parameters\")\n",
    "model = ModelParams(parser, sentinel=True)\n",
    "pipeline = PipelineParams(parser)\n",
    "parser.add_argument('--target', default='scene', type=str)\n",
    "\n",
    "args = get_combined_args(parser, MODEL_PATH)\n",
    "\n",
    "dataset = model.extract(args)\n",
    "\n",
    "# If use language-driven segmentation, load clip feature and original masks\n",
    "dataset.need_features = False\n",
    "\n",
    "# To obtain mask scales\n",
    "dataset.need_masks = True\n",
    "\n",
    "scene_gaussians = GaussianModel(dataset.sh_degree)\n",
    "\n",
    "feature_gaussians = FeatureGaussianModel(FEATURE_DIM)\n",
    "scene = Scene(dataset, scene_gaussians, feature_gaussians, load_iteration=-1, feature_load_iteration=FEATURE_GAUSSIAN_ITERATION, shuffle=False, mode='eval', target='contrastive_feature')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "200a47a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import QuantileTransformer\n",
    "# Borrowed from GARField, but modified\n",
    "def get_quantile_func(scales: torch.Tensor, distribution=\"normal\"):\n",
    "    \"\"\"\n",
    "    Use 3D scale statistics to normalize scales -- use quantile transformer.\n",
    "    \"\"\"\n",
    "    scales = scales.flatten()\n",
    "\n",
    "    scales = scales.detach().cpu().numpy()\n",
    "    print(scales.max(), '?')\n",
    "\n",
    "    # Calculate quantile transformer\n",
    "    quantile_transformer = QuantileTransformer(output_distribution=distribution)\n",
    "    quantile_transformer = quantile_transformer.fit(scales.reshape(-1, 1))\n",
    "\n",
    "    \n",
    "    def quantile_transformer_func(scales):\n",
    "        scales_shape = scales.shape\n",
    "\n",
    "        scales = scales.reshape(-1,1)\n",
    "        \n",
    "        return torch.Tensor(\n",
    "            quantile_transformer.transform(scales.detach().cpu().numpy())\n",
    "        ).to(scales.device).reshape(scales_shape)\n",
    "\n",
    "    return quantile_transformer_func, quantile_transformer\n",
    "    \n",
    "all_scales = []\n",
    "for cam in scene.getTrainCameras():\n",
    "    all_scales.append(cam.mask_scales)\n",
    "all_scales = torch.cat(all_scales)\n",
    "\n",
    "upper_bound_scale = all_scales.max().item()\n",
    "# upper_bound_scale = np.percentile(all_scales.detach().cpu().numpy(), 75)\n",
    "\n",
    "# all_scales = []\n",
    "# for cam in scene.getTrainCameras():\n",
    "#     cam.mask_scales = torch.clamp(cam.mask_scales, 0, upper_bound_scale).detach()\n",
    "#     all_scales.append(cam.mask_scales)\n",
    "# all_scales = torch.cat(all_scales)\n",
    "\n",
    "# quantile transformer\n",
    "q_trans, q_trans_ = get_quantile_func(all_scales, 'uniform')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "111bb186",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_scales.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For saving the normal q_trans, if the q_trans is uniform, we don't have to save it\n",
    "\n",
    "# from joblib import dump, load\n",
    "# dump(q_trans_, 'quantile_trans_test.jl')\n",
    "# a = load('quantile_trans_test.jl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deb3cc8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "upper_bound_scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30809275",
   "metadata": {},
   "outputs": [],
   "source": [
    "q_trans(torch.Tensor([70]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7d85228",
   "metadata": {},
   "source": [
    "# Begin Segmenting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f07ca2d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "cameras = scene.getTrainCameras()\n",
    "print(\"There are\",len(cameras),\"views in the dataset.\")\n",
    "print(upper_bound_scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b9d07b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_img_camera_id = 40\n",
    "mask_img_camera_id = 40\n",
    "\n",
    "view = deepcopy(cameras[ref_img_camera_id])\n",
    "\n",
    "view.feature_height, view.feature_width = view.image_height, view.image_width\n",
    "img = view.original_image * 255\n",
    "img = img.permute([1,2,0]).detach().cpu().numpy().astype(np.uint8)\n",
    "\n",
    "bg_color = [0 for i in range(FEATURE_DIM)]\n",
    "background = torch.tensor(bg_color, dtype=torch.float32, device=\"cuda\")\n",
    "rendered_feature = render_contrastive_feature(view, feature_gaussians, pipeline.extract(args), background, norm_point_features=True, smooth_type = None)['render']\n",
    "feature_h, feature_w = rendered_feature.shape[-2:]\n",
    "\n",
    "\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdc56249",
   "metadata": {},
   "source": [
    "# Point Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cddb348",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    # If the q_trans is normal\n",
    "    # scale = 2.\n",
    "    # scale = torch.full((1,), scale).cuda()\n",
    "    # scale = q_trans(scale)\n",
    "\n",
    "    # If the q_trans is uniform, the scale can be any value between 0 and 1\n",
    "    # scale = torch.tensor([0]).cuda()\n",
    "    # scale = torch.tensor([0.5]).cuda()\n",
    "    scale = torch.tensor([0.6]).cuda()\n",
    "\n",
    "    gates = scale_gate(scale)\n",
    "\n",
    "    feature_with_scale = rendered_feature\n",
    "    feature_with_scale = feature_with_scale * gates.unsqueeze(-1).unsqueeze(-1)\n",
    "    scale_conditioned_feature = feature_with_scale.permute([1,2,0])\n",
    "\n",
    "    plt.imshow(scale_conditioned_feature[:,:,:3].detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "312b919b",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_index = (180, 430)\n",
    "\n",
    "query_index = (\n",
    "    int(query_index[0] / view.image_height * view.feature_height),\n",
    "    int(query_index[1] / view.image_width * view.feature_width),\n",
    "               )\n",
    "\n",
    "normed_features = torch.nn.functional.normalize(scale_conditioned_feature, dim = -1, p = 2)\n",
    "query_feature = normed_features[query_index[0], query_index[1]]\n",
    "\n",
    "similarity = torch.einsum('C,HWC->HW', query_feature, normed_features)\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "plt.imshow(similarity.detach().cpu().numpy())\n",
    "plt.subplot(1,2,2)\n",
    "plt.imshow(similarity.detach().cpu().numpy() > 0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee611885",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SHOW PCA in 2D\n",
    "\n",
    "# # CHW -> PCA dimension reduction DHW (D=3)\n",
    "\n",
    "# from sklearn.decomposition import PCA\n",
    "\n",
    "# pca = PCA(n_components=3)\n",
    "\n",
    "\n",
    "# # t = pca.fit_transform(torch.nn.functional.normalize(rendered_feature, dim = 0).reshape(rendered_feature.shape[0], -1).permute([1,0]).cpu().detach().numpy())\n",
    "# t = pca.fit_transform(normed_features.reshape(-1, normed_features.shape[-1]).cpu().detach().numpy())\n",
    "\n",
    "# t = (t - t.min(axis=0)) / (t.max(axis=0)-t.min(axis=0))\n",
    "# t = t.reshape(normed_features.shape[0], normed_features.shape[1], 3)\n",
    "# plt.imshow(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4272df9",
   "metadata": {},
   "source": [
    "# Cluster in 2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50a9cbec",
   "metadata": {},
   "outputs": [],
   "source": [
    "downsampled_features = torch.nn.functional.interpolate(scale_conditioned_feature.permute([2,0,1]).unsqueeze(0), (128, 128), mode = 'bilinear').squeeze()\n",
    "cluster_normed_features = torch.nn.functional.normalize(downsampled_features, dim = 0, p = 2).permute([1,2,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3080f26f",
   "metadata": {},
   "outputs": [],
   "source": [
    "clusterer = hdbscan.HDBSCAN(min_cluster_size=10, cluster_selection_epsilon=0.01)\n",
    "cluster_labels = clusterer.fit_predict(cluster_normed_features.reshape([-1, cluster_normed_features.shape[-1]]).detach().cpu().numpy())\n",
    "labels = cluster_labels.reshape([cluster_normed_features.shape[0], cluster_normed_features.shape[1]])\n",
    "print(np.unique(labels))\n",
    "\n",
    "cluster_centers = torch.zeros(len(np.unique(labels))-1, cluster_normed_features.shape[-1])\n",
    "for i in range(1, len(np.unique(labels))):\n",
    "    cluster_centers[i-1] = torch.nn.functional.normalize(cluster_normed_features[labels == i-1].mean(dim = 0), dim = -1)\n",
    "\n",
    "label_to_color = np.random.rand(200, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd3c7e4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "label_to_color = np.random.rand(200, 3)\n",
    "segmentation_res = torch.einsum('nc,hwc->hwn', cluster_centers.cuda(), normed_features)\n",
    "\n",
    "segmentation_res_idx = segmentation_res.argmax(dim = -1)\n",
    "colored_labels = label_to_color[segmentation_res_idx.cpu().numpy().astype(np.int8)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f5268fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(colored_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00096329",
   "metadata": {},
   "source": [
    "# Segmentation in 3D"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9caaec85",
   "metadata": {},
   "source": [
    "After obtaining query features, we can use it to segment objects in 3D."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c350d58b",
   "metadata": {},
   "outputs": [],
   "source": [
    "point_features = feature_gaussians.get_point_features\n",
    "\n",
    "scale_conditioned_point_features = point_features * gates.unsqueeze(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b68145e",
   "metadata": {},
   "outputs": [],
   "source": [
    "normed_scale_conditioned_point_features = torch.nn.functional.normalize(scale_conditioned_point_features, dim = -1, p = 2)\n",
    "\n",
    "similarities = torch.einsum('C,NC->N', query_feature.cuda(), normed_scale_conditioned_point_features)\n",
    "\n",
    "similarities[similarities < 0.5] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11f6646d",
   "metadata": {},
   "outputs": [],
   "source": [
    "bg_color = [0 for i in range(FEATURE_DIM)]\n",
    "background = torch.tensor(bg_color, dtype=torch.float32, device=\"cuda\")\n",
    "rendered_similarities = render(cameras[ref_img_camera_id], scene_gaussians, pipeline.extract(args), background, override_color=similarities.unsqueeze(-1).repeat([1,3]))['render']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce2f6344",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(rendered_similarities.permute([1,2,0])[:,:,0].detach().cpu() > 0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dfc3069",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    scene_gaussians.roll_back()\n",
    "except:\n",
    "    pass\n",
    "scene_gaussians.segment(similarities > 0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12afe761",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the segmentation\n",
    "name = 'precomputed_mask'\n",
    "import os\n",
    "os.makedirs('./segmentation_res', exist_ok=True)\n",
    "torch.save(similarities > 0.75, f'./segmentation_res/{name}.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37eea5fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "bg_color = [1 for i in range(FEATURE_DIM)]\n",
    "background = torch.tensor(bg_color, dtype=torch.float32, device=\"cuda\")\n",
    "rendered_segmented_image = render(cameras[ref_img_camera_id], scene_gaussians, pipeline.extract(args), background)['render']\n",
    "plt.imshow(rendered_segmented_image.permute([1,2,0]).detach().cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b098510b",
   "metadata": {},
   "outputs": [],
   "source": [
    "scene_gaussians.roll_back()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f68a361b",
   "metadata": {},
   "source": [
    "# Cluster in 3D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e55301a",
   "metadata": {},
   "outputs": [],
   "source": [
    "point_features = feature_gaussians.get_point_features\n",
    "scale_conditioned_point_features = torch.nn.functional.normalize(point_features, dim = -1, p = 2) * gates.unsqueeze(0)\n",
    "normed_point_features = torch.nn.functional.normalize(scale_conditioned_point_features, dim = -1, p = 2)\n",
    "sampled_point_features = scale_conditioned_point_features[torch.rand(scale_conditioned_point_features.shape[0]) > 0.98]\n",
    "normed_sampled_point_features = sampled_point_features / torch.norm(sampled_point_features, dim = -1, keepdim = True)\n",
    "\n",
    "print(len(sampled_point_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47efb599",
   "metadata": {},
   "outputs": [],
   "source": [
    "clusterer = cuml.HDBSCAN(min_cluster_size=10, cluster_selection_epsilon=0.01)\n",
    "\n",
    "cluster_labels = clusterer.fit_predict(normed_sampled_point_features.detach().cpu().numpy())\n",
    "print(np.unique(cluster_labels))\n",
    "\n",
    "cluster_centers = torch.zeros(len(np.unique(cluster_labels))-1, normed_sampled_point_features.shape[-1])\n",
    "for i in range(1, len(np.unique(cluster_labels))):\n",
    "    cluster_centers[i-1] = torch.nn.functional.normalize(normed_sampled_point_features[cluster_labels == i-1].mean(dim = 0), dim = -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75276b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "seg_score = torch.einsum('nc,bc->bn', cluster_centers.cpu(), normed_point_features.cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65a79feb",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_to_color = np.random.rand(1000, 3)\n",
    "point_colors = label_to_color[seg_score.argmax(dim = -1).cpu().numpy()]\n",
    "point_colors[seg_score.max(dim = -1)[0].detach().cpu().numpy() < 0.5] = (0,0,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f21cf12f",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    scene_gaussians.roll_back()\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3fee177",
   "metadata": {},
   "outputs": [],
   "source": [
    "bg_color = [0 for i in range(FEATURE_DIM)]\n",
    "background = torch.tensor(bg_color, dtype=torch.float32, device=\"cuda\")\n",
    "\n",
    "rendered_seg_map = render(cameras[ref_img_camera_id], scene_gaussians, pipeline.extract(args), background, override_color=torch.from_numpy(point_colors).cuda().float())['render']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f3a00e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(rendered_seg_map.permute([1,2,0]).detach().cpu().numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a2502b3",
   "metadata": {},
   "source": [
    "# Language-driven Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8978b82a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample some anchor points in 3D\n",
    "anchor_point_features = feature_gaussians.get_point_features[torch.rand(feature_gaussians.get_point_features.shape[0]) > 0.99]\n",
    "print(len(anchor_point_features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f17e2868",
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "seg_features = []\n",
    "clip_features = []\n",
    "scales = []\n",
    "mask_identifiers = []\n",
    "camera_id_mask_id = []\n",
    "\n",
    "# vote_weights = []\n",
    "\n",
    "bg_color = [0 for i in range(FEATURE_DIM)]\n",
    "background = torch.tensor(bg_color, dtype=torch.float32, device=\"cuda\")\n",
    "\n",
    "i=0\n",
    "for view in scene.getTrainCameras():\n",
    "    torch.cuda.empty_cache()\n",
    "    clip_features.append(view.original_features)\n",
    "    tmp_view = deepcopy(view)\n",
    "    tmp_view.feature_height, tmp_view.feature_width = view.original_image.shape[-2:]\n",
    "\n",
    "    rendered_feature = render_contrastive_feature(tmp_view, feature_gaussians, pipeline.extract(args), background, norm_point_features=True)['render']\n",
    "    feature_h, feature_w = rendered_feature.shape[-2:]\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # NOTE: 3D-OVS need 8x downsample since its images are in 4K resolution\n",
    "        # rendered_feature = torch.nn.functional.interpolate(rendered_feature.unsqueeze(0), (feature_h // 8, feature_w // 8), mode = 'bilinear').squeeze()\n",
    "        # sam_masks = view.original_masks.cuda().unsqueeze(1)\n",
    "        # sam_masks = torch.nn.functional.interpolate(sam_masks, (feature_h // 8, feature_w // 8), mode = 'bilinear')\n",
    "\n",
    "        rendered_feature = torch.nn.functional.interpolate(rendered_feature.unsqueeze(0), (feature_h // 4, feature_w // 4), mode = 'bilinear').squeeze()\n",
    "        sam_masks = view.original_masks.cuda().unsqueeze(1)\n",
    "        sam_masks = torch.nn.functional.interpolate(sam_masks.float(), (feature_h // 4, feature_w // 4), mode = 'bilinear')\n",
    "\n",
    "\n",
    "        sam_masks = torch.conv2d(\n",
    "            sam_masks.float().cpu(),\n",
    "            torch.full((3, 3), 1.0).view(1, 1, 3, 3).cpu(),\n",
    "            padding=1,\n",
    "        )\n",
    "        sam_masks = sam_masks >= 2\n",
    "        sam_masks = sam_masks.cuda()\n",
    "\n",
    "\n",
    "        # vote_weights is the number of pixels in the mask\n",
    "        # vote_weights.append(sam_masks.sum(dim = -1).sum(dim = -1))\n",
    "\n",
    "        # sam_masks = torch.nn.functional.interpolate(sam_masks.unsqueeze(1), (view.feature_height, view.feature_width), mode = 'bilinear')\n",
    "        # sam_masks[sam_masks > 0.75] = 1\n",
    "        # sam_masks[sam_masks != 1] = 0\n",
    "\n",
    "        mask_scales = view.mask_scales.cuda().unsqueeze(-1)\n",
    "        mask_scales = q_trans(mask_scales)\n",
    "\n",
    "        scale_gates = scale_gate(mask_scales)\n",
    "        \n",
    "        # int_sampled_scales = ((1 - mask_scales.squeeze()) * 10).long()\n",
    "        # scale_gates = fixed_scale_gate[int_sampled_scales].detach()\n",
    "\n",
    "        # N_scale N_anchor C\n",
    "        # scale_conditioned_anchor_point_features = torch.einsum('nc,mc->nmc',scale_gates, anchor_point_features)\n",
    "        scale_conditioned_anchor_point_features = scale_gates.unsqueeze(1) * anchor_point_features.unsqueeze(0)\n",
    "        scale_conditioned_anchor_point_features = torch.nn.functional.normalize(scale_conditioned_anchor_point_features, dim = -1, p = 2)\n",
    "\n",
    "        # N_scale C H W\n",
    "        scale_conditioned_feature = rendered_feature.unsqueeze(0) * scale_gates.unsqueeze(-1).unsqueeze(-1)\n",
    "        scale_conditioned_feature = torch.nn.functional.normalize(scale_conditioned_feature, dim = 1, p = 2)\n",
    "\n",
    "        # N_scale C\n",
    "        mask_features = (sam_masks * scale_conditioned_feature).sum(dim = -1).sum(dim = -1) / (sam_masks.sum(dim = -1).sum(dim = -1) + 1e-9)\n",
    "        mask_features = torch.nn.functional.normalize(mask_features, dim = -1, p = 2)\n",
    "\n",
    "        mask_identifier = torch.einsum('nmc,nc->nm', scale_conditioned_anchor_point_features, mask_features) > 0.5\n",
    "        \n",
    "        mask_identifiers.append(mask_identifier.cpu())\n",
    "        seg_features.append(mask_features)\n",
    "        scales.append(view.mask_scales.cuda().unsqueeze(-1))\n",
    "\n",
    "        for j in range(len(mask_features)):\n",
    "            camera_id_mask_id.append((i, j))\n",
    "        i+=1\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "804c2399",
   "metadata": {},
   "outputs": [],
   "source": [
    "flattened_mask_features = torch.cat(seg_features, dim = 0)\n",
    "flattened_clip_features = torch.cat(clip_features, dim = 0)\n",
    "flattened_clip_features = torch.nn.functional.normalize(flattened_clip_features.float(), dim = -1, p = 2)\n",
    "flattened_scales = torch.cat(scales, dim = 0)\n",
    "flattened_mask_identifiers = torch.cat(mask_identifiers, dim = 0).to(torch.float16).cuda()\n",
    "flattened_mask_features.shape, flattened_clip_features.shape, flattened_scales.shape, flattened_mask_identifiers.shape, len(camera_id_mask_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce2be51",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with torch.no_grad():\n",
    "    distance_map = 1 - (torch.einsum('mc,nc->mn', flattened_mask_identifiers, flattened_mask_identifiers) / (flattened_mask_identifiers.sum(dim = -1).unsqueeze(-1) + flattened_mask_identifiers.sum(dim = -1).unsqueeze(0) - torch.einsum('mc,nc->mn', flattened_mask_identifiers, flattened_mask_identifiers) + 1e-6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13fb860c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct vote graph\n",
    "\n",
    "from hdbscan import HDBSCAN\n",
    "\n",
    "clusterer = HDBSCAN(min_cluster_size=30, cluster_selection_epsilon=0.25, metric='precomputed')\n",
    "cluster_labels = clusterer.fit_predict(distance_map.detach().cpu().numpy().astype(np.float64))\n",
    "cluster_labels = torch.from_numpy(cluster_labels).to(device = flattened_clip_features.device, dtype = torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ee6d771",
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib\n",
    "import clip_utils\n",
    "importlib.reload(clip_utils)\n",
    "from clip_utils import get_scores_with_template\n",
    "from clip_utils.clip_utils import load_clip\n",
    "clip_model = load_clip()\n",
    "clip_model.eval()\n",
    "1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7242b420",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert input text prompts to scores for multi-view masks\n",
    "\n",
    "scores = get_scores_with_template(clip_model, flattened_clip_features.cuda(), \"lamp\")\n",
    "scores = scores.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d23057c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_scores = torch.zeros(len(torch.unique(cluster_labels)), device=cluster_labels.device)\n",
    "for cluster_idx in torch.unique(cluster_labels):\n",
    "    cluster_scores[cluster_idx+1] = scores[cluster_labels == cluster_idx].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa63e79e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_id_to_scales(cluster_labels, flattened_scales, cluster_idx, scores):\n",
    "    max_score_mask_scale_id = scores[cluster_labels == cluster_idx].argmax()\n",
    "    return flattened_scales[cluster_labels == cluster_idx][max_score_mask_scale_id].item(), max_score_mask_scale_id\n",
    "\n",
    "cluster_labels.unique(), cluster_labels.unique()[cluster_scores.argmax()], cluster_scores.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68e16cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "good_clusters = torch.where(cluster_scores > 0.45)[0]\n",
    "good_cluster_scores = cluster_scores[good_clusters]\n",
    "if len(good_clusters) != 0:\n",
    "    print(len(good_clusters))\n",
    "    good_clusters = [cluster_labels.unique()[i] for i in good_clusters]\n",
    "else:\n",
    "    good_clusters = [cluster_labels.unique()[cluster_scores.argmax()]]\n",
    "\n",
    "clip_query_features = []\n",
    "corresponding_scales = []\n",
    "\n",
    "for g in good_clusters:\n",
    "\n",
    "    s, ind = cluster_id_to_scales(cluster_labels, flattened_scales, g, scores)\n",
    "    clip_query_features.append(torch.nn.functional.normalize(flattened_mask_features[cluster_labels == g][ind], dim = -1, p = 2))\n",
    "\n",
    "    corresponding_scales.append(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6588272c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable\n",
    "def get_similarity_map(point_features:torch.Tensor, scale:float, scale_gate:Callable, clip_query_feature:torch.Tensor, q_trans:Callable[[torch.Tensor], torch.Tensor]):\n",
    "    # scale = 0.5\n",
    "    scale = torch.full((1,), scale).cuda()\n",
    "    scale = q_trans(scale)\n",
    "\n",
    "    gates = scale_gate(scale).detach().squeeze()\n",
    "\n",
    "    print(point_features.shape, gates.shape)\n",
    "\n",
    "    scale_conditioned_point_features = point_features * gates.unsqueeze(0)\n",
    "\n",
    "    normed_scale_conditioned_point_features = torch.nn.functional.normalize(scale_conditioned_point_features, dim = -1, p = 2)\n",
    "\n",
    "    similarities = torch.einsum('C,NC->N', clip_query_feature, normed_scale_conditioned_point_features)\n",
    "\n",
    "    return similarities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e3c92c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 0\n",
    "similarities = get_similarity_map(feature_gaussians.get_point_features, corresponding_scales[index], scale_gate, clip_query_features[index], q_trans)\n",
    "good_cluster_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0f5924b",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    scene_gaussians.roll_back()\n",
    "except:\n",
    "    pass\n",
    "scene_gaussians.segment(similarities > 0.85)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18fb012f",
   "metadata": {},
   "outputs": [],
   "source": [
    "bg_color = [1 for i in range(FEATURE_DIM)]\n",
    "background = torch.tensor(bg_color, dtype=torch.float32, device=\"cuda\")\n",
    "rendered_segmented_image = render(cameras[ref_img_camera_id], scene_gaussians, pipeline.extract(args), background)['render']\n",
    "plt.imshow(rendered_segmented_image.permute([1,2,0]).detach().cpu())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a0ab473",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
